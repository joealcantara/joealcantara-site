+++
date = '2025-12-23T12:37:47Z'
draft = false
title = 'About'
+++

I'm an AI safety researcher with interests spanning technical AI safety and the geopolitical dimensions of AI infrastructure.

## Research Interests

### 1. Scientist AI: Interpretable World Models Without Agency

Can we build safer, more interpretable AI through passive causal world model learning? I'm exploring how removing agency (goal-seeking behavior) while retaining learning capability can create inherently safer AI systems. This involves implementing Scientist AI at scale to demonstrate that interpretable causal world models are both practical and safer than black-box optimization.

### 2. Few-Shot Catastrophe Prevention

When we catch AI misbehavior, what's the best way to use that information to prevent future failures? I'm investigating how to detect and prevent AI catastrophes using learned causal structures from limited examples, building systematic benchmarks to compare prevention techniques like probes, fine-tuning, and monitoring.

### 3. Infrastructure Sovereignty & Digital Vassalage

Can nations achieve "Sovereign AI" without owning the physical or cloud infrastructure? I'm examining the power dynamics between nation-states and the cloud provider triopoly (AWS/Microsoft/Google), comparing the UK's "Tiered Sovereignty" model with the Philippines' "Client-State" model to understand infrastructure as territory and the implications of cloud rent-seeking.

## Current Work

I'm currently working on Paper 1: A benchmark framework for evaluating catastrophe prevention techniques. This work builds on pilot studies that validated the research direction and identified a 4-paper research arc.

## Background

- PhD researcher in AI Safety (Interpretable World Models, Catastrophe Prevention, Infrastructure Sovereignty)
- Teaching Fellow at University of Warwick
- SFHEA (Senior Fellow of the Higher Education Academy) candidate

## Contact

- Email: jomar.alcantara@gmail.com
- GitHub: [joealcantara](https://github.com/joealcantara)

---

*This site shares research progress, pilot studies, and mini-articles on AI safety. All work is pre-publication and subject to revision.*
